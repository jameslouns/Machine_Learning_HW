{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "A5.5_Classification_with_Convolutions.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zSwfMQfEy75W",
        "cvcYP5NBy75X"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2VFiQA1y73j"
      },
      "source": [
        "# A5 Classification with CNNs\n",
        "\n",
        "In this assignment, you will define a new class named `NeuralNetworkClassifierCNN` that extends the `NeuralNetworkClassifier` class provided here. You will compare how your `NeuralNetworkClassifier` comapres with your `NeuralNetworkClassifierCNN` to train a classifier of Images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIB9uTYqy73o"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxfL3ySZzR13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57334cbb-9114-4806-d017-f14ebdc0bba5"
      },
      "source": [
        "%%writefile optimizers.py\n",
        "import numpy as np\n",
        "\n",
        "######################################################################\n",
        "## class Optimizers()\n",
        "######################################################################\n",
        "\n",
        "class Optimizers():\n",
        "\n",
        "    def __init__(self, all_weights):\n",
        "        '''all_weights is a vector of all of a neural networks weights concatenated into a one-dimensional vector'''\n",
        "        \n",
        "        self.all_weights = all_weights\n",
        "\n",
        "        # The following initializations are only used by adam.\n",
        "        # Only initializing m, v, beta1t and beta2t here allows multiple calls to adam to handle training\n",
        "        # with multiple subsets (batches) of training data.\n",
        "        self.mt = np.zeros_like(all_weights)\n",
        "        self.vt = np.zeros_like(all_weights)\n",
        "        self.beta1 = 0.9\n",
        "        self.beta2 = 0.999\n",
        "        self.beta1t = 1\n",
        "        self.beta2t = 1\n",
        "\n",
        "        \n",
        "    def sgd(self, error_f, gradient_f, fargs=[], n_epochs=100, learning_rate=0.001, verbose=True, error_convert_f=None):\n",
        "        '''\n",
        "error_f: function that requires X and T as arguments (given in fargs) and returns mean squared error.\n",
        "gradient_f: function that requires X and T as arguments (in fargs) and returns gradient of mean squared error\n",
        "            with respect to each weight.\n",
        "error_convert_f: function that converts the standardized error from error_f to original T units.\n",
        "        '''\n",
        "\n",
        "        error_trace = []\n",
        "        epochs_per_print = n_epochs // 10\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "\n",
        "            error = error_f(*fargs)\n",
        "            grad = gradient_f(*fargs)\n",
        "\n",
        "            # Update all weights using -= to modify their values in-place.\n",
        "            self.all_weights -= learning_rate * grad\n",
        "\n",
        "            if error_convert_f:\n",
        "                error = error_convert_f(error)\n",
        "            error_trace.append(error)\n",
        "\n",
        "            if verbose and ((epoch + 1) % max(1, epochs_per_print) == 0):\n",
        "                print(f'sgd: Epoch {epoch+1:d} Error={error:.5f}')\n",
        "\n",
        "        return error_trace\n",
        "\n",
        "    def adam(self, error_f, gradient_f, fargs=[], n_epochs=100, learning_rate=0.001, verbose=True, error_convert_f=None):\n",
        "        '''\n",
        "error_f: function that requires X and T as arguments (given in fargs) and returns mean squared error.\n",
        "gradient_f: function that requires X and T as arguments (in fargs) and returns gradient of mean squared error\n",
        "            with respect to each weight.\n",
        "error_convert_f: function that converts the standardized error from error_f to original T units.\n",
        "        '''\n",
        "\n",
        "        alpha = learning_rate  # learning rate called alpha in original paper on adam\n",
        "        epsilon = 1e-8\n",
        "        error_trace = []\n",
        "        epochs_per_print = n_epochs // 10\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "\n",
        "            error = error_f(*fargs)\n",
        "            grad = gradient_f(*fargs)\n",
        "\n",
        "            self.mt[:] = self.beta1 * self.mt + (1 - self.beta1) * grad\n",
        "            self.vt[:] = self.beta2 * self.vt + (1 - self.beta2) * grad * grad\n",
        "            self.beta1t *= self.beta1\n",
        "            self.beta2t *= self.beta2\n",
        "\n",
        "            m_hat = self.mt / (1 - self.beta1t)\n",
        "            v_hat = self.vt / (1 - self.beta2t)\n",
        "\n",
        "            # Update all weights using -= to modify their values in-place.\n",
        "            self.all_weights -= alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n",
        "    \n",
        "            if error_convert_f:\n",
        "                error = error_convert_f(error)\n",
        "            error_trace.append(error)\n",
        "\n",
        "            if verbose and ((epoch + 1) % max(1, epochs_per_print) == 0):\n",
        "                print(f'Adam: Epoch {epoch+1:d} Error={error:.5f}')\n",
        "\n",
        "        return error_trace\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.ion()\n",
        "\n",
        "    def parabola(wmin):\n",
        "        return ((w - wmin) ** 2)[0]\n",
        "\n",
        "    def parabola_gradient(wmin):\n",
        "        return 2 * (w - wmin)\n",
        "\n",
        "    w = np.array([0.0])\n",
        "    optimizer = Optimizers(w)\n",
        "\n",
        "    wmin = 5\n",
        "    optimizer.sgd(parabola, parabola_gradient, [wmin],\n",
        "                  n_epochs=500, learning_rate=0.1)\n",
        "\n",
        "    print(f'sgd: Minimum of parabola is at {wmin}. Value found is {w}')\n",
        "\n",
        "    w = np.array([0.0])\n",
        "    optimizer = Optimizers(w)\n",
        "    optimizer.adam(parabola, parabola_gradient, [wmin],\n",
        "                   n_epochs=500, learning_rate=0.1)\n",
        "    \n",
        "    print(f'adam: Minimum of parabola is at {wmin}. Value found is {w}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing optimizers.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwByRR1ly-vW"
      },
      "source": [
        "import optimizers\n",
        "import sys  # for sys.float_info.epsilon\n",
        "\n",
        "######################################################################\n",
        "## class NeuralNetwork()\n",
        "######################################################################\n",
        "\n",
        "class NeuralNetwork():\n",
        "\n",
        "\n",
        "    def __init__(self, n_inputs, n_hiddens_per_layer, n_outputs, activation_function='tanh'):\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_outputs = n_outputs\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "        # Set self.n_hiddens_per_layer to [] if argument is 0, [], or [0]\n",
        "        if n_hiddens_per_layer == 0 or n_hiddens_per_layer == [] or n_hiddens_per_layer == [0]:\n",
        "            self.n_hiddens_per_layer = []\n",
        "        else:\n",
        "            self.n_hiddens_per_layer = n_hiddens_per_layer\n",
        "\n",
        "        # Initialize weights, by first building list of all weight matrix shapes.\n",
        "        n_in = n_inputs\n",
        "        shapes = []\n",
        "        for nh in self.n_hiddens_per_layer:\n",
        "            shapes.append((n_in + 1, nh))\n",
        "            n_in = nh\n",
        "        shapes.append((n_in + 1, n_outputs))\n",
        "\n",
        "        # self.all_weights:  vector of all weights\n",
        "        # self.Ws: list of weight matrices by layer\n",
        "        self.all_weights, self.Ws = self.make_weights_and_views(shapes)\n",
        "\n",
        "        # Define arrays to hold gradient values.\n",
        "        # One array for each W array with same shape.\n",
        "        self.all_gradients, self.dE_dWs = self.make_weights_and_views(shapes)\n",
        "\n",
        "        self.trained = False\n",
        "        self.total_epochs = 0\n",
        "        self.error_trace = []\n",
        "        self.Xmeans = None\n",
        "        self.Xstds = None\n",
        "        self.Tmeans = None\n",
        "        self.Tstds = None\n",
        "\n",
        "\n",
        "    def make_weights_and_views(self, shapes):\n",
        "        # vector of all weights built by horizontally stacking flatenned matrices\n",
        "        # for each layer initialized with uniformly-distributed values.\n",
        "        all_weights = np.hstack([np.random.uniform(size=shape).flat / np.sqrt(shape[0])\n",
        "                                 for shape in shapes])\n",
        "        # Build list of views by reshaping corresponding elements from vector of all weights\n",
        "        # into correct shape for each layer.\n",
        "        views = []\n",
        "        start = 0\n",
        "        for shape in shapes:\n",
        "            size =shape[0] * shape[1]\n",
        "            views.append(all_weights[start:start + size].reshape(shape))\n",
        "            start += size\n",
        "        return all_weights, views\n",
        "\n",
        "\n",
        "    # Return string that shows how the constructor was called\n",
        "    def __repr__(self):\n",
        "        return f'{type(self).__name__}({self.n_inputs}, {self.n_hiddens_per_layer}, {self.n_outputs}, \\'{self.activation_function}\\')'\n",
        "\n",
        "\n",
        "    # Return string that is more informative to the user about the state of this neural network.\n",
        "    def __str__(self):\n",
        "        result = self.__repr__()\n",
        "        if len(self.error_trace) > 0:\n",
        "            return self.__repr__() + f' trained for {len(self.error_trace)} epochs, final training error {self.error_trace[-1]:.4f}'\n",
        "\n",
        "\n",
        "    def train(self, X, T, n_epochs, learning_rate, method='sgd', verbose=True):\n",
        "        '''\n",
        "train: \n",
        "  X: n_samples x n_inputs matrix of input samples, one per row\n",
        "  T: n_samples x n_outputs matrix of target output values, one sample per row\n",
        "  n_epochs: number of passes to take through all samples updating weights each pass\n",
        "  learning_rate: factor controlling the step size of each update\n",
        "  method: is either 'sgd' or 'adam'\n",
        "        '''\n",
        "\n",
        "        # Setup standardization parameters\n",
        "        if self.Xmeans is None:\n",
        "            self.Xmeans = X.mean(axis=0)\n",
        "            self.Xstds = X.std(axis=0)\n",
        "            self.Xstds[self.Xstds == 0] = 1  # So we don't divide by zero when standardizing\n",
        "            self.Tmeans = T.mean(axis=0)\n",
        "            self.Tstds = T.std(axis=0)\n",
        "            \n",
        "        # Standardize X and T\n",
        "        X = (X - self.Xmeans) / self.Xstds\n",
        "        T = (T - self.Tmeans) / self.Tstds\n",
        "\n",
        "        # Instantiate Optimizers object by giving it vector of all weights\n",
        "        optimizer = optimizers.Optimizers(self.all_weights)\n",
        "\n",
        "        # Define function to convert value from error_f into error in original T units, \n",
        "        # but only if the network has a single output. Multiplying by self.Tstds for \n",
        "        # multiple outputs does not correctly unstandardize the error.\n",
        "        if len(self.Tstds) == 1:\n",
        "            error_convert_f = lambda err: (np.sqrt(err) * self.Tstds)[0] # to scalar\n",
        "        else:\n",
        "            error_convert_f = lambda err: np.sqrt(err)[0] # to scalar\n",
        "            \n",
        "\n",
        "        if method == 'sgd':\n",
        "\n",
        "            error_trace = optimizer.sgd(self.error_f, self.gradient_f,\n",
        "                                        fargs=[X, T], n_epochs=n_epochs,\n",
        "                                        learning_rate=learning_rate,\n",
        "                                        verbose=True,\n",
        "                                        error_convert_f=error_convert_f)\n",
        "\n",
        "        elif method == 'adam':\n",
        "\n",
        "            error_trace = optimizer.adam(self.error_f, self.gradient_f,\n",
        "                                         fargs=[X, T], n_epochs=n_epochs,\n",
        "                                         learning_rate=learning_rate,\n",
        "                                         verbose=True,\n",
        "                                         error_convert_f=error_convert_f)\n",
        "\n",
        "        else:\n",
        "            raise Exception(\"method must be 'sgd' or 'adam'\")\n",
        "        \n",
        "        self.error_trace = error_trace\n",
        "\n",
        "        # Return neural network object to allow applying other methods after training.\n",
        "        #  Example:    Y = nnet.train(X, T, 100, 0.01).use(X)\n",
        "        return self\n",
        "\n",
        "    def relu(self, s):\n",
        "        s[s < 0] = 0\n",
        "        return s\n",
        "\n",
        "    def grad_relu(self, s):\n",
        "        return (s > 0).astype(int)\n",
        "    \n",
        "    def forward_pass(self, X):\n",
        "        '''X assumed already standardized. Output returned as standardized.'''\n",
        "        self.Ys = [X]\n",
        "        for W in self.Ws[:-1]:\n",
        "            if self.activation_function == 'relu':\n",
        "                self.Ys.append(self.relu(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\n",
        "            else:\n",
        "                self.Ys.append(np.tanh(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\n",
        "        last_W = self.Ws[-1]\n",
        "        self.Ys.append(self.Ys[-1] @ last_W[1:, :] + last_W[0:1, :])\n",
        "        return self.Ys\n",
        "\n",
        "    # Function to be minimized by optimizer method, mean squared error\n",
        "    def error_f(self, X, T):\n",
        "        Ys = self.forward_pass(X)\n",
        "        mean_sq_error = np.mean((T - Ys[-1]) ** 2)\n",
        "        return mean_sq_error\n",
        "\n",
        "    # Gradient of function to be minimized for use by optimizer method\n",
        "    def gradient_f(self, X, T):\n",
        "        '''Assumes forward_pass just called with layer outputs in self.Ys.'''\n",
        "        error = T - self.Ys[-1]\n",
        "        n_samples = X.shape[0]\n",
        "        n_outputs = T.shape[1]\n",
        "        delta = - error / (n_samples * n_outputs)\n",
        "        n_layers = len(self.n_hiddens_per_layer) + 1\n",
        "        # Step backwards through the layers to back-propagate the error (delta)\n",
        "        for layeri in range(n_layers - 1, -1, -1):\n",
        "            # gradient of all but bias weights\n",
        "            self.dE_dWs[layeri][1:, :] = self.Ys[layeri].T @ delta\n",
        "            # gradient of just the bias weights\n",
        "            self.dE_dWs[layeri][0:1, :] = np.sum(delta, 0)\n",
        "            # Back-propagate this layer's delta to previous layer\n",
        "            if self.activation_function == 'relu':\n",
        "                delta = delta @ self.Ws[layeri][1:, :].T * self.grad_relu(self.Ys[layeri])\n",
        "            else:\n",
        "                delta = delta @ self.Ws[layeri][1:, :].T * (1 - self.Ys[layeri] ** 2)\n",
        "        return self.all_gradients\n",
        "\n",
        "    def use(self, X):\n",
        "        '''X assumed to not be standardized'''\n",
        "        # Standardize X\n",
        "        X = (X - self.Xmeans) / self.Xstds\n",
        "        Ys = self.forward_pass(X)\n",
        "        Y = Ys[-1]\n",
        "        # Unstandardize output Y before returning it\n",
        "        return Y * self.Tstds + self.Tmeans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GWN5pW9y73t"
      },
      "source": [
        "class NeuralNetworkClassifier(NeuralNetwork):\n",
        "\n",
        "  def makeIndicatorVars(self, T):\n",
        "      # Make sure T is two-dimensional. Should be nSamples x 1.\n",
        "      if T.ndim == 1:\n",
        "          T = T.reshape((-1, 1))\n",
        "      retT = (T == np.unique(T)).astype(int)\n",
        "      return retT\n",
        "      \n",
        "  def softmax (self, X):\n",
        "      fs = np.exp(X)  # N x K\n",
        "      denom = np.sum(fs, axis=1).reshape((-1, 1))\n",
        "      gs = fs / (denom + sys.float_info.epsilon)\n",
        "      return gs\n",
        "\n",
        "  def train(self, X, T, n_epochs, learning_rate, method='sgd', verbose=True):\n",
        "        '''\n",
        "train: \n",
        "  X: n_samples x n_inputs matrix of input samples, one per row\n",
        "  T: n_samples x n_outputs matrix of target output values, one sample per row\n",
        "  n_epochs: number of passes to take through all samples updating weights each pass\n",
        "  learning_rate: factor controlling the step size of each update\n",
        "  method: is either 'sgd' or 'adam'\n",
        "        '''\n",
        "\n",
        "        # Setup standardization parameters\n",
        "        if self.Xmeans is None:\n",
        "            self.Xmeans = X.mean(axis=0)\n",
        "            self.Xstds = X.std(axis=0)\n",
        "            self.Xstds[self.Xstds == 0] = 1  # So we don't divide by zero when standardizing\n",
        "            self.Tmeans = T.mean(axis=0)\n",
        "            self.Tstds = T.std(axis=0)\n",
        "            \n",
        "        # Standardize X and T\n",
        "        X = (X - self.Xmeans) / self.Xstds\n",
        "        self.TtrainI = self.makeIndicatorVars(T)\n",
        "        self.uniqueT=np.unique(T)\n",
        "        # Instantiate Optimizers object by giving it vector of all weights\n",
        "        optimizer = optimizers.Optimizers(self.all_weights)\n",
        "\n",
        "        # Define function to convert value from error_f into error in original T units, \n",
        "        # but only if the network has a single output. Multiplying by self.Tstds for \n",
        "        # multiple outputs does not correctly unstandardize the error.\n",
        "        error_convert_f = lambda nll: (np.exp(-nll))\n",
        "\n",
        "        if method == 'sgd':\n",
        "\n",
        "            error_trace = optimizer.sgd(self.error_f, self.gradient_f,\n",
        "                                        fargs=[X, self.TtrainI], n_epochs=n_epochs,\n",
        "                                        learning_rate=learning_rate,\n",
        "                                        verbose=True,\n",
        "                                        error_convert_f=error_convert_f)\n",
        "\n",
        "        elif method == 'adam':\n",
        "\n",
        "            error_trace = optimizer.adam(self.error_f, self.gradient_f,\n",
        "                                         fargs=[X, self.TtrainI], n_epochs=n_epochs,\n",
        "                                         learning_rate=learning_rate,\n",
        "                                         verbose=True,\n",
        "                                         error_convert_f=error_convert_f)\n",
        "\n",
        "        else:\n",
        "            raise Exception(\"method must be 'sgd' or 'adam'\")\n",
        "        \n",
        "        self.error_trace = error_trace\n",
        "\n",
        "        # Return neural network object to allow applying other methods after training.\n",
        "        #  Example:    Y = nnet.train(X, T, 100, 0.01).use(X)\n",
        "        return self\n",
        "\n",
        "    # Function to be minimized by optimizer method, mean squared error  \n",
        "  def error_f(self, X, T):\n",
        "      temp = self.forward_pass(X)\n",
        "      Y = self.softmax(temp[-1])\n",
        "      #mean_sq_error = np.mean((T - Ys[-1]) ** 2)\n",
        "      return - np.mean((T * np.log(Y)))\n",
        "\n",
        "\n",
        "  # Gradient of function to be minimized for use by optimizer method \n",
        "  # Look at Lecture 10\n",
        "  def gradient_f(self, X, T):\n",
        "      Y = self.softmax(self.Ys[-1])\n",
        "      delta = (Y - T) / (T.shape[0] * T.shape[1])\n",
        "      n_layers = len(self.n_hiddens_per_layer) + 1\n",
        "      for layeri in range(n_layers - 1, -1, -1):\n",
        "            # gradient of all but bias weights\n",
        "            self.dE_dWs[layeri][1:, :] = self.Ys[layeri].T @ delta\n",
        "            # gradient of just the bias weights\n",
        "            self.dE_dWs[layeri][0:1, :] = np.sum(delta, 0)\n",
        "            # Back-propagate this layer's delta to previous layer\n",
        "            if self.activation_function == 'relu':\n",
        "                delta = delta @ self.Ws[layeri][1:, :].T * self.grad_relu(self.Ys[layeri])\n",
        "            else:\n",
        "                delta = delta @ self.Ws[layeri][1:, :].T * (1 - self.Ys[layeri] ** 2)\n",
        "      return self.all_gradients\n",
        "\n",
        "  def use(self, X):\n",
        "      Xtest = ((X - self.Xmeans)/self.Xstds)\n",
        "      #Xtest1 = np.hstack(( np.ones((Xtest.shape[0],1)), Xtest))\n",
        "      temp = self.forward_pass(Xtest)\n",
        "      logregOutput = self.softmax(temp[-1])\n",
        "      predictedTrain = np.argmax(logregOutput,axis=1)\n",
        "      result = np.array(list(map(lambda x: [self.uniqueT[x].astype(int)] , predictedTrain)))\n",
        "      return result , logregOutput"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-c1pXsNy73y"
      },
      "source": [
        "class NeuralNetworkClassifierCNN(NeuralNetworkClassifier):\n",
        "      def __init__(self, n_inputs, n_hiddens_per_layer, n_outputs, patch_size, stride, activation_function='tanh'):\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_outputs = n_outputs\n",
        "        self.activation_function = activation_function\n",
        "        self.stride = stride\n",
        "        self.patch_size = patch_size \n",
        "\n",
        "        # Set self.n_hiddens_per_layer to [] if argument is 0, [], or [0]\n",
        "        if n_hiddens_per_layer == 0 or n_hiddens_per_layer == [] or n_hiddens_per_layer == [0]:\n",
        "            self.n_hiddens_per_layer = []\n",
        "        else:\n",
        "            self.n_hiddens_per_layer = n_hiddens_per_layer\n",
        "\n",
        "        # Initialize weights, by first building list of all weight matrix shapes.\n",
        "        n_in = n_inputs\n",
        "        shapes = []\n",
        "        # First build shape of weight matrix for convolutional layer.  Only one allowed.\n",
        "        shapes = [(self.patch_size * self.patch_size + 1, n_hiddens_per_layer[0])]\n",
        "        input_size = int(np.sqrt(n_inputs))\n",
        "        n_in = ((input_size - self.patch_size) // self.stride + 1) ** 2 * self.n_hiddens_per_layer[0]\n",
        "        for nh in self.n_hiddens_per_layer[1:]:\n",
        "            shapes.append((n_in + 1, nh))\n",
        "            n_in = nh\n",
        "        shapes.append((n_in + 1, self.n_outputs))\n",
        "\n",
        "        # self.all_weights:  vector of all weights\n",
        "        # self.Ws: list of weight matrices by layer\n",
        "        self.all_weights, self.Ws = self.make_weights_and_views(shapes)\n",
        "\n",
        "        # Define arrays to hold gradient values.\n",
        "        # One array for each W array with same shape.\n",
        "        self.all_gradients, self.dE_dWs = self.make_weights_and_views(shapes)\n",
        "\n",
        "        self.trained = False\n",
        "        self.total_epochs = 0\n",
        "        self.error_trace = []\n",
        "        self.Xmeans = None\n",
        "        self.Xstds = None\n",
        "        self.Tmeans = None\n",
        "        self.Tstds = None\n",
        "\n",
        "      def _make_patches(self, X, patch_size, stride=1):\n",
        "        '''X: n_samples x n_pixels  (flattened square images)'''\n",
        "        X = np.ascontiguousarray(X)  # make sure X values are contiguous in memory\n",
        "        n_samples = X.shape[0]\n",
        "        image_size = int(np.sqrt(X.shape[1]))\n",
        "        n_patches = (image_size - patch_size ) // stride + 1\n",
        "        nb = X.itemsize  # number of bytes each value\n",
        "        new_shape = [n_samples, n_patches, n_patches, patch_size, patch_size]\n",
        "        new_strides = [image_size * image_size * nb,\n",
        "                        image_size * stride * nb,\n",
        "                        stride * nb,\n",
        "                        image_size * nb,\n",
        "                        nb]\n",
        "        X = np.lib.stride_tricks.as_strided(X, shape=new_shape, strides=new_strides)\n",
        "        X = X.reshape(n_samples, n_patches * n_patches, patch_size * patch_size)\n",
        "        return X\n",
        "\n",
        "      def train(self, X, T, n_epochs, learning_rate, method='sgd', verbose=True):\n",
        "          '''\n",
        "          train: \n",
        "          X: n_samples x n_inputs matrix of input samples, one per row\n",
        "          T: n_samples x n_outputs matrix of target output values, one sample per row\n",
        "          n_epochs: number of passes to take through all samples updating weights each pass\n",
        "          learning_rate: factor controlling the step size of each update\n",
        "          method: is either 'sgd' or 'adam'\n",
        "          '''\n",
        "\n",
        "          # Setup standardization parameters\n",
        "          if self.Xmeans is None:\n",
        "              self.Xmeans = X.mean(axis=0)\n",
        "              self.Xstds = X.std(axis=0)\n",
        "              self.Xstds[self.Xstds == 0] = 1  # So we don't divide by zero when standardizing\n",
        "              self.Tmeans = T.mean(axis=0)\n",
        "              self.Tstds = T.std(axis=0)\n",
        "              \n",
        "          # Standardize X and T\n",
        "          X = (X - self.Xmeans) / self.Xstds\n",
        "          self.TtrainI = self.makeIndicatorVars(T)\n",
        "          self.uniqueT=np.unique(T)\n",
        "          # Instantiate Optimizers object by giving it vector of all weights\n",
        "          optimizer = optimizers.Optimizers(self.all_weights)\n",
        "\n",
        "          # Define function to convert value from error_f into error in original T units, \n",
        "          # but only if the network has a single output. Multiplying by self.Tstds for \n",
        "          # multiple outputs does not correctly unstandardize the error.\n",
        "          error_convert_f = lambda nll: (np.exp(-nll))\n",
        "\n",
        "          X_patches = self._make_patches(X, self.patch_size, self.stride)\n",
        "\n",
        "          if method == 'sgd':\n",
        "\n",
        "              error_trace = optimizer.sgd(self.error_f, self.gradient_f,\n",
        "                                          fargs=[X_patches, self.TtrainI], n_epochs=n_epochs,\n",
        "                                          learning_rate=learning_rate,\n",
        "                                          verbose=True,\n",
        "                                          error_convert_f=error_convert_f)\n",
        "\n",
        "          elif method == 'adam':\n",
        "\n",
        "              error_trace = optimizer.adam(self.error_f, self.gradient_f,\n",
        "                                          fargs=[X_patches, self.TtrainI], n_epochs=n_epochs,\n",
        "                                          learning_rate=learning_rate,\n",
        "                                          verbose=True,\n",
        "                                          error_convert_f=error_convert_f)\n",
        "\n",
        "          else:\n",
        "              raise Exception(\"method must be 'sgd' or 'adam'\")\n",
        "          \n",
        "          self.error_trace = error_trace\n",
        "\n",
        "          # Return neural network object to allow applying other methods after training.\n",
        "          #  Example:    Y = nnet.train(X, T, 100, 0.01).use(X)\n",
        "          return self\n",
        "\n",
        "      def forward_pass(self, X_patches):\n",
        "        '''X assumed already standardized. Output returned as standardized.'''\n",
        "        self.Ys = [X_patches]\n",
        "        for layer_i, W in enumerate(self.Ws[:-1]):\n",
        "            if self.activation_function == 'relu':\n",
        "                self.Ys.append(self.relu(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\n",
        "            else:\n",
        "                self.Ys.append(np.tanh(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\n",
        "            # If convolutional layer, flatten each sample into vector for input to following f\n",
        "            # fully-connected layer.\n",
        "            if layer_i == 0:\n",
        "                self.Ys[-1] = self.Ys[-1].reshape(self.Ys[-1].shape[0], -1)\n",
        "        last_W = self.Ws[-1]\n",
        "        self.Ys.append(self.Ys[-1] @ last_W[1:, :] + last_W[0:1, :])\n",
        "        return self.Ys\n",
        "\n",
        "      def gradient_f(self, X_patches, T):\n",
        "        Y = self.softmax(self.Ys[-1])\n",
        "        delta = (Y - T) / (T.shape[0] * T.shape[1])\n",
        "        n_layers = len(self.n_hiddens_per_layer) + 1\n",
        "        for layeri in range(n_layers - 1, -1, -1):\n",
        "              if layeri == 0:\n",
        "                  # Convolutional layer\n",
        "                  # delta, backpropagated from a fully-connected layer, has multiple values for each\n",
        "                  # convolutional unit, for each application of it to each patch.  We must sum the dE_dWs\n",
        "                  # for all of those delta values by multiplying each delta value for each convolutional\n",
        "                  # unit by the patch values used to produce the output by the input values for the \n",
        "                  # corresponding patch. \n",
        "                  # Do this by first reshaping the backed-up delta matrix to the right form.\n",
        "                  patch_n_values = X_patches.shape[-1]\n",
        "                  n_conv_units = self.n_hiddens_per_layer[0]\n",
        "                  delta_reshaped = delta.reshape(-1, n_conv_units)\n",
        "                  # And we must reshape the convolutional layer input matrix to a compatible shape.\n",
        "                  conv_layer_inputs_reshaped = self.Ys[0].reshape(-1, patch_n_values)\n",
        "                  # Now we can calculate the dE_dWs for the convolutional layer with a simple matrix\n",
        "                  # multiplication.\n",
        "                  self.dE_dWs[layeri][1:, :] = conv_layer_inputs_reshaped.T @ delta_reshaped\n",
        "                  self.dE_dWs[layeri][0:1, :] = np.sum(delta_reshaped, axis=0)\n",
        "              else:\n",
        "                  # Fully-connected layers\n",
        "                  # gradient of all but bias weights\n",
        "                  self.dE_dWs[layeri][1:, :] = self.Ys[layeri].T @ delta\n",
        "                  # gradient of just the bias weights\n",
        "                  self.dE_dWs[layeri][0:1, :] = np.sum(delta, 0)\n",
        "                  # Back-propagate this layer's delta to previous layer\n",
        "                  if self.activation_function == 'relu':\n",
        "                      delta = delta @ self.Ws[layeri][1:, :].T * self.grad_relu(self.Ys[layeri])\n",
        "                  else:\n",
        "                      delta = delta @ self.Ws[layeri][1:, :].T * (1 - self.Ys[layeri] ** 2)\n",
        "        return self.all_gradients\n",
        "\n",
        "      def use(self, X):\n",
        "        '''X assumed to not be standardized. Returns (classes, class_probabilities)'''\n",
        "        # Standardize X\n",
        "        X = (X - self.Xmeans) / self.Xstds\n",
        "        # Convert flattened samples into patches\n",
        "        X_patches = self._make_patches(X, self.patch_size, self.stride)\n",
        "        Ys = self.forward_pass(X_patches)\n",
        "        Y = self.softmax(Ys[-1])\n",
        "        classes = np.array(list(map(lambda x: [self.uniqueT[x.astype(int)].astype(int)] , Y)))\n",
        "        return classes, Y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aqy3v2TM0uXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23f3d611-ff53-4506-afa0-4da57c0343b1"
      },
      "source": [
        "!curl -O http://deeplearning.net/data/mnist/mnist.pkl.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:26 --:--:--     0^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7m-xde_y731",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e524fb5-b041-4ac9-d8f6-619fc01184ca"
      },
      "source": [
        "import pickle\n",
        "import gzip\n",
        "\n",
        "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
        "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
        "    \n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "\n",
        "Xtrain = train_set[0]\n",
        "Ttrain = train_set[1].reshape(-1, 1)\n",
        "\n",
        "Xval = valid_set[0]\n",
        "Tval = valid_set[1].reshape(-1, 1)\n",
        "\n",
        "Xtest = test_set[0]\n",
        "Ttest = test_set[1].reshape(-1, 1)\n",
        "\n",
        "print(Xtrain.shape, Ttrain.shape,  Xval.shape, Tval.shape,  Xtest.shape, Ttest.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 784) (50000, 1) (10000, 784) (10000, 1) (10000, 784) (10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwLDpS9xy736",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8e4568f-3d5d-4b75-fc08-68b31a078f35"
      },
      "source": [
        "import time \n",
        "\n",
        "n_epochs = 100\n",
        "learning_rate = 0.01\n",
        "\n",
        "np.random.seed(142)\n",
        "start_time = time.time()\n",
        "\n",
        "nnet = NeuralNetworkClassifierCNN(Xtrain.shape[1], [2], len(np.unique(Ttrain)),\n",
        "                                       patch_size=5, stride=2)\n",
        "nnet.train(Xtrain, Ttrain, n_epochs, learning_rate, method='adam', verbose=True)\n",
        "\n",
        "print(f'took {time.time() - start_time} seconds')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adam: Epoch 10 Error=0.94528\n",
            "Adam: Epoch 20 Error=0.95903\n",
            "Adam: Epoch 30 Error=0.96421\n",
            "Adam: Epoch 40 Error=0.96684\n",
            "Adam: Epoch 50 Error=0.96858\n",
            "Adam: Epoch 60 Error=0.96982\n",
            "Adam: Epoch 70 Error=0.97083\n",
            "Adam: Epoch 80 Error=0.97174\n",
            "Adam: Epoch 90 Error=0.97258\n",
            "Adam: Epoch 100 Error=0.97335\n",
            "took 342.44557547569275 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxKKp2yZy73-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43055074-820c-4239-cef5-b78e76e277b4"
      },
      "source": [
        "print(nnet)  # uses the __str__ method"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NeuralNetworkClassifierCNN(784, [2], 10, 'tanh') trained for 100 epochs, final training error 0.9734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUkfMo-Hy74C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "63510e5e-d889-48eb-9a14-4c845f3836dc"
      },
      "source": [
        "plt.plot(nnet.error_trace);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGX5O6nry74G"
      },
      "source": [
        "classes_test, probs_test = nnet.use(Xtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM9oPbbly74K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "627f1706-49f7-4dcb-9f2f-35b11a39ff09"
      },
      "source": [
        "confusion_matrix(classes_test, Ttest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-10e00f7f304e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-_lM40Wy74O"
      },
      "source": [
        "## CIFAR-10\n",
        "Great, now, load a subset of the CIFAR-10 data and train your network again with the hyperparameters that worked best for MNIST. How well do the parameters work for this new dataset? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJBxCOExbodl"
      },
      "source": [
        "# **I couldnt finish this part because I couldnt find the full address to curl the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEc9CzF_y74O"
      },
      "source": [
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMTwi0oey74S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "7168d9fd-fe9e-4ead-99b8-651342aafb4b"
      },
      "source": [
        "data_batch_1 = unpickle('cifar-10-batches-py/data_batch_1')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-f0bbb11905a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_batch_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cifar-10-batches-py/data_batch_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-f420c7a33123>\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bytes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cifar-10-batches-py/data_batch_1'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv5i4jj_y74V",
        "outputId": "a0f7f932-a69b-4500-9a15-2ef3eb4f4f8c"
      },
      "source": [
        "data_batch_1.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_XjH4o0y74Z"
      },
      "source": [
        "Xtrain = data_batch_1[b'data']\n",
        "Ttrain = np.array(data_batch_1[b'labels']).reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvHfRu_Qy74b",
        "outputId": "a07eae7a-f9e1-4a12-f6fb-67a2f884bf2b"
      },
      "source": [
        "image0 = Xtrain[0, :]\n",
        "image0 = image0.reshape(3, 32, 32)\n",
        "image0 = np.rollaxis(image0, 0, 3)\n",
        "image0.shape\n",
        "\n",
        "plt.imshow(image0);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pVGhangy74f"
      },
      "source": [
        "Xtrain = Xtrain.reshape(-1, 3, 32, 32)\n",
        "Xtrain = np.rollaxis(Xtrain, 1, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P--1qCn2y74i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHs07V5Py74m",
        "outputId": "f12fb81f-eef2-4362-dad7-d6fd9b0d7e5b"
      },
      "source": [
        "Xtrain.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JykM6Dj6y74p"
      },
      "source": [
        "def grayscale(data, dtype='float32'):\n",
        "    # luma coding weighted average in video systems\n",
        "    r, g, b = np.asarray(.3, dtype=dtype), np.asarray(.59, dtype=dtype), np.asarray(.11, dtype=dtype)\n",
        "    rst = r * data[:, :, :, 0] + g * data[:, :, :, 1] + b * data[:, :, :, 2]\n",
        "    # add channel dimension\n",
        "    rst = np.expand_dims(rst, axis=3)\n",
        "    return rst\n",
        "\n",
        "X_train_gray = grayscale(Xtrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJR3EQeey74s",
        "outputId": "96a70692-85f7-43f0-9a0c-142d48bac88f"
      },
      "source": [
        "image0 = X_train_gray[0, :].reshape(32,32)\n",
        "\n",
        "plt.imshow(-image0, cmap='gray');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_INWSY9ny74v"
      },
      "source": [
        "test_batch = unpickle('cifar-10-batches-py/test_batch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrNCicpLy74z"
      },
      "source": [
        "Xtest = test_batch[b'data']\n",
        "Ttest = np.array(test_batch[b'labels']).reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsJp7wtey742"
      },
      "source": [
        "Xtest = Xtest.reshape(-1, 3, 32, 32)\n",
        "Xtest = np.rollaxis(Xtest, 1, 4)\n",
        "X_test_gray = grayscale(Xtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RUZwaeuy745",
        "outputId": "7613e82b-821b-4e98-c93a-27834426d264"
      },
      "source": [
        "X_train_gray.shape, Ttrain.shape, X_test_gray.shape, Ttrain.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 32, 32, 1), (10000, 1), (10000, 32, 32, 1), (10000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCUJf_8Vy748"
      },
      "source": [
        "X_train_gray= X_train_gray.reshape(-1,32 * 32)\n",
        "X_test_gray = X_test_gray.reshape(-1,32 * 32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXtnr_S7y74_",
        "outputId": "8e6a2baa-c359-4273-8b79-d7d7ecb08752"
      },
      "source": [
        "X_train_gray.shape, Ttrain.shape, X_test_gray.shape, Ttrain.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 1024), (10000, 1), (10000, 1024), (10000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCIwSIzvy75C",
        "outputId": "790046c5-b721-4841-c5a5-b7901dfa0272"
      },
      "source": [
        "X_train_gray.shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWmhFI8Fy75G",
        "outputId": "3e62ed12-fdf5-49be-bdda-8574e97083b5"
      },
      "source": [
        "len(np.unique(Ttrain))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz5zYUdCy75J",
        "outputId": "a0d5506b-91e8-4dbb-a95e-f73c0bd5c191"
      },
      "source": [
        "nnet = NeuralNetworkClassifierCNN(X_train_gray.shape[1], [20, 20], len(np.unique(Ttrain)),\n",
        "                                       patch_size=2, stride=2)\n",
        "nnet.train(X_train_gray, Ttrain, n_epochs, learning_rate, method='adam', verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adam: Epoch 10 Error=0.81352\n",
            "Adam: Epoch 20 Error=0.82172\n",
            "Adam: Epoch 30 Error=0.82694\n",
            "Adam: Epoch 40 Error=0.83118\n",
            "Adam: Epoch 50 Error=0.83459\n",
            "Adam: Epoch 60 Error=0.83709\n",
            "Adam: Epoch 70 Error=0.83975\n",
            "Adam: Epoch 80 Error=0.84173\n",
            "Adam: Epoch 90 Error=0.84433\n",
            "Adam: Epoch 100 Error=0.84626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNetworkClassifierCNN(1024, [20, 20], 10, 2, 2, 'tanh')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ4tzvHqy75N",
        "outputId": "a5ea1e9e-3325-48f3-fc9f-5f58e8a5d86f"
      },
      "source": [
        "X_train_gray.shape, Ttrain.shape, X_test_gray.shape, Ttrain.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000, 1024), (10000, 1), (10000, 1024), (10000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqlBdwUGy75P"
      },
      "source": [
        "cifar_predictions, cifar_probs = nnet.use(X_test_gray)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbckzE5py75S",
        "outputId": "4319f5f4-b7c2-4be6-e0a3-70ad7a90141d"
      },
      "source": [
        "confusion_matrix(cifar_predictions, Ttest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25.8</td>\n",
              "      <td>3.4</td>\n",
              "      <td>14.8</td>\n",
              "      <td>3.7</td>\n",
              "      <td>10.4</td>\n",
              "      <td>6.6</td>\n",
              "      <td>7.1</td>\n",
              "      <td>7.0</td>\n",
              "      <td>15.6</td>\n",
              "      <td>5.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.8</td>\n",
              "      <td>30.2</td>\n",
              "      <td>4.6</td>\n",
              "      <td>6.1</td>\n",
              "      <td>4.1</td>\n",
              "      <td>2.5</td>\n",
              "      <td>9.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>11.9</td>\n",
              "      <td>23.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>26.9</td>\n",
              "      <td>6.7</td>\n",
              "      <td>15.9</td>\n",
              "      <td>7.8</td>\n",
              "      <td>14.4</td>\n",
              "      <td>7.7</td>\n",
              "      <td>4.8</td>\n",
              "      <td>2.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>11.5</td>\n",
              "      <td>14.1</td>\n",
              "      <td>12.2</td>\n",
              "      <td>15.9</td>\n",
              "      <td>13.0</td>\n",
              "      <td>9.7</td>\n",
              "      <td>6.5</td>\n",
              "      <td>6.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>19.6</td>\n",
              "      <td>7.3</td>\n",
              "      <td>27.5</td>\n",
              "      <td>6.6</td>\n",
              "      <td>12.2</td>\n",
              "      <td>8.9</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6.0</td>\n",
              "      <td>2.1</td>\n",
              "      <td>11.2</td>\n",
              "      <td>12.9</td>\n",
              "      <td>11.5</td>\n",
              "      <td>24.2</td>\n",
              "      <td>11.2</td>\n",
              "      <td>8.4</td>\n",
              "      <td>9.0</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4.8</td>\n",
              "      <td>7.7</td>\n",
              "      <td>10.1</td>\n",
              "      <td>9.5</td>\n",
              "      <td>11.0</td>\n",
              "      <td>9.7</td>\n",
              "      <td>31.7</td>\n",
              "      <td>5.1</td>\n",
              "      <td>4.9</td>\n",
              "      <td>5.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6.7</td>\n",
              "      <td>4.8</td>\n",
              "      <td>11.6</td>\n",
              "      <td>9.3</td>\n",
              "      <td>13.4</td>\n",
              "      <td>6.2</td>\n",
              "      <td>7.6</td>\n",
              "      <td>27.2</td>\n",
              "      <td>7.3</td>\n",
              "      <td>5.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>11.6</td>\n",
              "      <td>8.2</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.2</td>\n",
              "      <td>4.3</td>\n",
              "      <td>7.6</td>\n",
              "      <td>4.4</td>\n",
              "      <td>6.5</td>\n",
              "      <td>39.1</td>\n",
              "      <td>9.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4.3</td>\n",
              "      <td>16.1</td>\n",
              "      <td>2.6</td>\n",
              "      <td>4.3</td>\n",
              "      <td>3.3</td>\n",
              "      <td>2.4</td>\n",
              "      <td>6.4</td>\n",
              "      <td>6.9</td>\n",
              "      <td>14.3</td>\n",
              "      <td>39.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0     1     2     3     4     5     6     7     8     9\n",
              "0  25.8   3.4  14.8   3.7  10.4   6.6   7.1   7.0  15.6   5.6\n",
              "1   2.8  30.2   4.6   6.1   4.1   2.5   9.5   5.0  11.9  23.3\n",
              "2  10.0   3.2  26.9   6.7  15.9   7.8  14.4   7.7   4.8   2.6\n",
              "3   7.2   3.5  11.5  14.1  12.2  15.9  13.0   9.7   6.5   6.4\n",
              "4   6.7   3.0  19.6   7.3  27.5   6.6  12.2   8.9   5.4   2.8\n",
              "5   6.0   2.1  11.2  12.9  11.5  24.2  11.2   8.4   9.0   3.5\n",
              "6   4.8   7.7  10.1   9.5  11.0   9.7  31.7   5.1   4.9   5.5\n",
              "7   6.7   4.8  11.6   9.3  13.4   6.2   7.6  27.2   7.3   5.9\n",
              "8  11.6   8.2   5.0   4.2   4.3   7.6   4.4   6.5  39.1   9.1\n",
              "9   4.3  16.1   2.6   4.3   3.3   2.4   6.4   6.9  14.3  39.4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSwfMQfEy75W"
      },
      "source": [
        "## Grading and Check-In\n",
        "\n",
        "When ready, submit your notebook via the A4 link in our class Canvas web page. Your notebook must be named as Lastname-A4.ipynb.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvcYP5NBy75X"
      },
      "source": [
        "## Extra Credit\n",
        "Earn 10 extra credit point on this assignment by doing the following.\n",
        "\n",
        "1. Add the additional CIFAR-10 data into your training. Do things improve? Worsen? (5 points)\n",
        "2. Train an equivalent CNN using pytorch, utilizing the GPU. Compare your accuracy and the speedup. (5 points) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GscbnRTy75X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3M1A9xRy75a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}